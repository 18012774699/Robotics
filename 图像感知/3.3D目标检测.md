# 3D目标检测

## 综述

2D Object Detection 的研究已经非常成熟了，代表作品有RPN系列的FasterRCNN和MaskRCNN，One Shot系列的YOLOv1-YOLOv3。这里推荐一个2D Object Detection发展过程和论文的[github链接](https://github.com/hoya012/deep_learning_object_detection)。

在2D Object Detection的基础上又提出了新的要求3D Object Detection。问题的具体描述检测环境中的三维物体，并给出物体的Bounding Box。相比于2D，3D的Bounding Box的表示除了多了一个维度的位置和尺寸，还多了三个角度。可以想象，一架飞机的Bounding Box的尺寸的是固定的，飞机的姿态除了位置之外，还有**俯仰角、偏航角和翻滚角**三个角度。

目前对于3D Object Detection有迫切需求的产业是自动驾驶产业，因为要想安全的自动驾驶，需要周围障碍物的三维位姿，在图片中的二维位姿不带深度信息，没有办法有效避免碰撞。所以3D Object Detection的数据集大多也是自动驾驶数据集，类别也主要是车辆和行人等，比较常用的有[KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)和[kaist](https://irap.kaist.ac.kr/dataset/)。由于自动驾驶针对车辆，所以障碍物的高度的检测对于安全行驶并没有十分重要，而障碍物都在陆地上，所以也不存在俯仰角和翻滚角两个角度。所以有些3D Object Detection方法将这三值忽略了（剩下**3维坐标和偏航角**）。

接下来我将详细列几篇论文，以及我认为论文中比较关键的一些点。每篇论文具体的细节和解读还请读者自行搜索。3D Object Detection的方法很大程度上是借鉴了2D Object Detection的方法。


## 数据格式

其数据格式包括 image、point cloud、RGBD、mesh 等。

来源：https://blog.csdn.net/weixin_39079670/article/details/84099559

**RGBD**

深度数据：深度图像的外观效果和灰度图像类似，但是深度数据表示的是每个像素点距离深度传感器的实际距离，深度图像和RGB图像的像素点是一一对应的。kinect的红外发射器不断向外发送红外结构光，同时红外接收器不断接收物体返回回来的红外信息，不同距离的物体返回的红外信息再红外接收器上表现出不同的强度，根据该原理得到的距离信息（深度数据），将不同距离的物体通过不同颜色来表示，就形成了深度图像。同时，由此引申出的RGB-D图像实际包括两部分:红绿蓝三通道图像（普通彩色图像）和深度图像。

**point cloud**

点云数据：点云数据指的是当一束激光照射在物体表面，所返回的数据信息中包括该物体表面各个点在三维空间中的坐标信息，这些点的组合就是激光点云，所得到的数据就是点云数据。深度数据和点云数据通过坐标变换可以互相转换得到，一般来说深度数据可以直接变换得到点云数据，但是点云数据需要有一定的规则和必要的相关信息才可以变换得到深度数据。一般来说，点云数据包括两大部分，points点坐标（X、Y、Z）和intensity信道中该点值的强度。

> points[i].x
>
> points[i].y
>
> points[i].z
>
> intensity[0].value[i]

![](.\image2\3D数据格式.png)

**点云数据处理方式**

此系列论文首先提出了一种新型的处理点云数据的深度学习模型-PointNet，并验证了它能够用于点云数据的多种认知任务，如**分类、语义分割和目标识别**。不同于图像数据在计算机中的表示通常编码了像素点之间的空间关系，点云数据由无序的数据点构成一个集合来表示。因此，在使用图像识别任务的深度学习模型处理点云数据之前，需要对点云数据进行一些处理。目前采用的方式主要有两种：

- **将点云数据投影到二维平面。**此种方式不直接处理三维的点云数据，而是先将点云投影到某些特定视角再处理，如前视视角和鸟瞰视角。同时，也可以融合使用来自相机的图像信息。通过将这些不同视角的数据相结合，来实现点云数据的认知任务。比较典型的算法有MV3D和AVOD。
- **将点云数据划分到有空间依赖关系的voxel。**此种方式通过分割三维空间，引入空间依赖关系到点云数据中，再使用3D卷积等方式来进行处理。这种方法的精度依赖于三维空间的分割细腻度，而且3D卷积的运算复杂度也较高。

不同于以上两种方法对点云数据先预处理再使用的方式，此论文的作者提出了**直接在点云数据上应用深度学习模型的方法**，称为PointNet。

点云数据详解：https://www.cnblogs.com/Libo-Master/p/9759130.html



## 数据集及格式分类？？

2D：COCO格式数据集：https://blog.csdn.net/qq_41185868/article/details/82939959



图像+点云

> ModelNet40
>
> [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)
>
> [kaist](https://irap.kaist.ac.kr/dataset/)
>
> SUN-RGBD
>
> ScanNet



## 算法分类

一、单目图像

- [x] YOLO-6D：Apollo
- [ ] SSD-6D



二、基于融合的方法RGB图像+激光雷达/深度图

- [x]  AVOD：激光+单目相机
- [x] MV3D-Net：RPN



三、基于激光雷达点云

- [x] Vote3Deep：点云数据栅格化，Sliding window，使用稀疏3D卷积（SCNN），[参考链接](https://blog.csdn.net/wqwqqwqw1231/article/details/90693612)

- [x] VoteNet
- [x] pointnet，pointnet++（分割）
- [ ] PointRCNN
- [x] Voxelnet



四、基于RGB-D图像

- [x] Frustum PointNets（目标检测）
- [ ] Frustum VoxNet for 3D object detection from RGB-D or Depth images



五、基于立体视觉



## 算法解析

### YOLO-6D

实际上就是物体在3D空间中的空间位置xyz，以及物体绕x轴，y轴和z轴旋转的角度。换言之，只要知道了物体在3D空间中的这六个自由度，就可以唯一确定物体的姿态。

*Real-Time Seamless Single Shot 6D Object Pose Prediction*这篇文章提出了一种使用一张2D图片来预测物体6D姿态的方法。**通过先预测3D bounding box在2D图像上的投影的1个中心点和8个角点，然后再由这9个点通过PNP算法计算得到6D姿态。**

![img](https://pic4.zhimg.com/80/v2-28ca0c53211495df96bf908316264a73_720w.jpg)

从上图可以看到，整个网络采用的是yolo v2的框架。网络吃一张2D的图片（a），吐出一个SxSx(9x2+1+C)的3D tensor(e)。我们会将原始输入图片划分成SxS个cell（c），物体的中心点落在哪个cell，哪个cell就负责预测这个物体的9个坐标点（9x2），confidence（1）以及类别(C)，这个思路和yolo是一样的。下面分别介绍这些输出的意义。



### SSD-6D





### PointCNN





### PointNet系列**

> 用于：分类、语义分割

来源： https://www.jiqizhixin.com/articles/2019-05-10-13 

**2.** **理论基础**

点云数据是在欧式空间下的点的一个子集，它具有以下三个特征：

- 无序。点云数据是一个集合，对数据的顺序是不敏感的。这就意味这处理点云数据的模型需要对数据的不同排列保持不变性。目前文献中使用的方法包括将无序的数据重排序、用数据的所有排列进行数据增强然后使用RNN模型、用对称函数来保证排列不变性。由于第三种方式的简洁性且容易在模型中实现，论文作者选择使用第三种方式，既使用maxpooling这个对称函数来提取点云数据的特征。
- 点与点之间的空间关系。一个物体通常由特定空间内的一定数量的点云构成，也就是说这些点云之间存在空间关系。为了能有效利用这种空间关系，论文作者提出了将局部特征和全局特征进行串联的方式来聚合信息。
- 不变性。点云数据所代表的目标对某些空间转换应该具有不变性，如旋转和平移。论文作者提出了在进行特征提取之前，先对点云数据进行对齐的方式来保证不变性。对齐操作是通过训练一个小型的网络来得到转换矩阵，并将之和输入点云数据相乘来实现。

 作者对他们模型进行了进一步的理论分析，并提出了两个定理： 

 **定理1，证明了PointNet的网络结构能够拟合任意的连续集合函数。其作用类似证明神经网络能够拟合任意连续函数一样【1】。**同时，作者发现PointNet模型的表征能力和maxpooling操作输出的数据维度(K)相关，K值越大，模型的表征能力越强。 

定理2(a)说明对于任何输入数据集S，都存在一个最小集Cs和一个最大集Ns，使得对Cs和Ns之间的任何集合T，其网络输出都和S一样。**这也就是说，模型对输入数据在有噪声(引入额外的数据点，趋于Ns)和有数据损坏(缺少数据点，趋于Cs)的情况都是鲁棒的。**定理2(b)说明了最小集Cs的数据多少由maxpooling操作输出数据的维度K给出上界。换个角度来讲，PointNet能够总结出表示某类物体形状的关键点，基于这些关键点PointNet能够判别物体的类别。这样的能力决定了PointNet对噪声和数据缺失的鲁棒性。如图所示，作者通过实验列出了PointNet学习到的以下几个物体的关键点。

![](.\image2\point-cloud rubost.png)

**3.** **PointNet系列模型结构**

**3.1 PointNet**

![](.\image2\PointNet.png)

PointNet的模型结构如上图所示，其关键流程介绍如下：

- 输入为一帧的全部点云数据的集合，表示为一个nx3的2d tensor，其中n代表点云数量，3对应xyz坐标。
- 输入数据先通过和一个T-Net学习到的转换矩阵相乘来对齐，保证了模型的对特定空间转换的不变性。
- 通过多次mlp（2层1d卷积）对各点云数据进行特征提取后，再用一个T-Net对特征进行对齐。
- 在特征的各个维度上执行maxpooling操作来得到最终的全局特征。
- 对分类任务，将全局特征通过mlp来预测最后的分类分数；对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过mlp得到每个数据点的分类结果。

Github：https://github.com/charlesq34/pointnet

**3.2 PointNet++**

在PointNet中, 直接对输入的点云数据整体进行卷积和max_pooling,**忽略了局部特征**. 且特征提取**忽略了密度不均匀的问题**, PointNet++解决了这2个问题.

Github：https://github.com/charlesq34/pointnet2

**3.2.1 网络构成**

**PointNet提取特征的方式是对所有点云数据提取了一个全局的特征，显然，这和目前流行的CNN逐层提取局部特征的方式不一样。**受到CNN的启发，作者提出了PointNet++，它能够在不同尺度提取局部特征，通过多层网络结构得到深层特征。PointNet++由以下几个关键部分构成：

- 采样层（sampling）：激光雷达单帧的数据点可以多达100k个，如果对每一个点都提取局部特征，计算量是非常巨大的。因此，作者提出了先对数据点进行采样。作者使用的采样算法是**最远点采样**（farthest point sampling, FPS），相对于随机采样，这种采样算法能够更好地覆盖整个采样空间。

> **FPS算法**: 随机选取一个点加入**中心点集合**, 之后选择离中心点集合里的点最远的点加入中心点集合中, 迭代选取中心点(后面选取的点需要和之前中心点集合中所有的点做距离计算**metric distance**),直到中心点集中点的**个数**达到阈值.

- 组合层（grouping）：为了提取一个点的局部特征，首先需要定义这个点的“**局部**”是什么。一个图片像素点的局部是其周围一定曼哈顿距离下的像素点，通常由卷积层的卷积核大小确定。同理，点云数据中的一个点的局部由其周围给定半径划出的球形空间内的其他点构成。组合层的作用就是找出通过采样层后的每一个点的所有构成其局部的点，以方便后续对每个局部提取特征。

> 该层的输入是原始点集N\*(d+C)和sampling出的中心点集N'\*d(N'是中心点个数,中心点只需要d坐标信息,不需要特征信息). 该层的输出是点集(point sets)的groups, 每个点集的shape是N'\*K\*(d+C), 每个group对应一个局部区域, 共有N'个局部区域, K是中心点周围的点的个数. 不同的group的K的值不一样. 虽然每个group含有的点的数量可能不同,但是使用pointnet结构提取出来的特征是维度一致的(每层特征图使用了全局max pooling)
>
> ∑N'*k = N

- 特征提取层（feature learning）：使用pointnet的网络提取局部区域的特征, 输入是grouping出的groups, 每个group的shape是N'\*K\*(d+C), 输出是N'\*(d+C'), 邻点的坐标减去中心点的坐标,作为他们的新坐标. 点特征shape C被embedding成shape C', K个邻点被抽象成一个特征。

上述各层构成了PointNet++的基础处理模块。如果将多个这样的处理模块级联组合起来，PointNet++就能像CNN一样从浅层特征得到深层语义特征。**对于分割任务的网络**，还需要将下采样后的特征进行上采样，使得原始点云中的每个点都有对应的特征。这个上采样的过程通过最近的k个临近点进行插值计算得到。完整的PointNet++的网络示意图如下图所示。

![img](.\image2\PointNet++.png)

**3.2.2 不均匀点云数据的特征提取**

不同于图片数据分布在规则的像素网格上且有**均匀的数据密度**，点云数据在空间中的分布是**不规则且不均匀**的。虽然PointNet能够用于对各个点云局部提取特征，但是由于点云在各个局部均匀性不一致，很可能导致学习到的PointNet不能提取到很好的局部特征。比如说，在越远的地方激光雷达数据通常变得越稀疏，因此**在稀疏的地方应该考虑更大的尺度范围来提取特征**。为此，作者提出了两种组合策略来保证更优的特征提取。

- 多尺度组合（multi-scale grouping, MSG）:比较直接的想法是对不同尺度的局部提取特征并将它们串联在一起，如下图(a)所示。但是因为需要对每个局部的每个尺度提取特征，其计算量的增加也是很显著的。
- 多分辨率组合（multi-resolution grouping, MRG）:为了解决MSG计算量太大的问题，作者提出了MRG。此种方法在某一层对每个局部提取到的特征由两个向量串联构成，如下图(b)所示。**第一部分由其前一层提取到的特征再次通过特征提取网络得到，第二部分则通过直接对这个局部对应的原始点云数据中的所有点进行特征提取得到。**

![](.\image2\不均匀点云数据的特征提取.png)

在这部分，作者还提到了一种random input dropout（DP）的方法，就是在输入到点云之前，对点集进行随机的Dropout,比例使用了95%，也就是说进行95%的重新采样。某种程度有点像数据增强，也是提高模型的robustness。那这些方法效果怎么样呢，我们一起来看一下。

从论文中的这幅分类实验结果图可以看出来，多尺度（MSG,MRG)和单一尺度相比（SSG）对分类的准确率没有什么提升，有一个好处是如果点云很稀疏的话，使用MSG可以保持很好的robustness。对于robustness效果random input dropout（DP）其实贡献更大。

![](.\image2\PN效果对比1.jpg)

从论文中的分割实验结果看，使用（MSG+DP）之后的确是比SSG结果提升了，在非均匀点云上差距会大一点，但是作者并没有给出MSG和DP对于效果提升单独的贡献对比，所以我们很难确定到底是MSG还是DP在这其中起作用了。

![](.\image2\PN效果对比2.jpg)

详见：https://blog.csdn.net/ygfrancois/article/details/89853854

详见：https://zhuanlan.zhihu.com/p/88238420

> 为何PointNet系列不能用来目标检测？个人认为，当点数比较少的时候，3D包围盒可能回归不准。

### Frustum-PointNet

详见：https://blog.csdn.net/ygfrancois/article/details/89853854

 https://www.jiqizhixin.com/articles/2019-05-10-13 

Github：https://github.com/charlesq34/frustum-pointnets

> 用于：实例分割、目标检测

PointNet和PointNet++基于3D点云数据做**分类和分割**，f-pointnets基于**RGB图像+深度信息**使用pointnet和pointnet++的结构做了**目标检测**。f-pointnets考虑了室内和室外的场景，基于KITTI数据集和SUN RGB-D 3D detection benchmarks数据集进行了训练。

使用到2D RGB图像的原因是当时基于纯3D点云数据的3D目标检测**对小目标检测效果不佳**，所以f-pointnets先基于2D RGB做2D的目标检测来**定位目标**，再基于2d目标检测结果**用其对应的点云数据视锥进行bbox回归**的方法来实现3D目标检测。使用纯3D的点云数据，计算量也会特别大，**效率**也是这个方法的优点之一。F-PointNet的网络结构如下图所示。

![img](.\image2\Frustum-PointNet.png)

可以看到，F-PointNet主要由以下几部分构成：

- 视锥生成（frustum proposal）：首先通过2D目标检测器来定位图片中的目标以及判断它们的类别。对每一个检测到的目标，通过标定好的传感器的内参和它们之间的转换矩阵得到其对应的点云数据中的各点，即**点云视锥**。作者使用的**2D目标检测**模型是基于VGG网络的FPN（Feature Pyramid Network）作为特征提取器，并用Fast R-CNN来预测最终的2D bbox。
- 3D实例分割（3D instance segmentation）:对每个得到的点云视锥，通过旋转得到以中心视角为坐标轴的**点云数据**。对转换后的点云数据用PointNet（或PointNet++）进行实例分割。实例分割是一个二分类问题，用于判断每个点属于某个目标或者不属于。
- 3D边界框回归（3D box estimation）:将上一步实例分割的结果作为mask得到属于某个实例的所有点云，计算其质心作为新的坐标系原点。通过一个T-Net进行回归得到**目标质心**和当前坐标原点的残差。将点云平移**（平移？）**到计算得到的目标质心后，通过PointNet（或PointNet++）对3D bbox的中心、尺寸和朝向进行回归得到最终的输出。此步骤采用的回归方式和Faster R-CNN中类似，不直接回归，而是回归到不同尺寸和朝向的锚点（anchors）。

> 此方法应该是，使用回归得到目标质心，代替了Faster R-CNN中撒点生成anchors的方式，提高了效率。

综上所述，F-PointNet是一个多步骤的3D目标检测算法。如下图所示，为了应对点云数据中各个目标的视角不变性和得到更准确的bbox回归（通过缩小需要回归的值的取值范围），算法需要进行三次坐标转换。模型的loss和2D的目标检测一样是包含分类以及回归的多任务loss。同时，作者提出了一种被称为corner loss的损失函数来对目标的中心、朝向和大小进行联合优化，避免由于某一方面的不准确而主导loss。

![img](.\image2\F-PointNet.png)

### PointRCNN

本文中提出了一种PointRCNN用于原始点云的3D目标检测，整个框架包括两个阶段：第一阶段使用自下而上的3D提案产生，第二阶段用于在规范坐标中修改提案获得最终的检测结果。

- Stage-1阶段子网络不是从RGB图像或者将点云投影到鸟类视图或者体素中，而是通过**将整个场景的点云分割为前景点和背景点**，以自下而上的方式直接从点云生成**少量高质量**的3D提案。
- Stage-2阶段子网络将每个提案的池化的点转换为规范坐标，更好地学习局部空间特征，这个过程与Stage-1中学习每个点的全局语义特征相结合，用于Box优化和置信度预测。

对KITTI数据集的3D检测基准的广泛实验表明，该架构优于只是用点云作为输入具有显著边缘的最先进方法。

代码：https://github.com/sshaoshuai/PointRCNN

来源：https://blog.csdn.net/taifengzikai/article/details/96840993

![](.\image2\PointRCNN.png)


### VoxelNet

> 点云数据，体素分区后，用于目标检测

https://blog.csdn.net/weixin_40805392/article/details/99549300

github：https://github.com/andylei77/VoxelNetRos

![](.\image2\VoxelNet.png)

### VoteNet

> 点云数据直接用于目标检测

https://blog.csdn.net/wqwqqwqw1231/article/details/101283243

https://zhuanlan.zhihu.com/p/63755003

https://zhuanlan.zhihu.com/p/98304645

![](.\image2\VoteNet1.png)



### MV3D-Net

![](.\image2\MV3D-Net.jpg)

> 用于：目标检测、分类

本文方法设计的具体步骤如下：

**1）提取特征**

**a. 提取点云俯视图特征**

**b. 提取点云前视图特征**

**c. 提取图像特征**

**2）从点云俯视图特征中计算候选区域**：俯视图由高度、强度、密度组成，投影到分辨率为0.1的二维网格中。

**3）把候选区域分别与1）中a、b、c得到的特征进行整合**

**a. 把俯视图候选区域投影到前视图和图像中**

**b. 经过ROI pooling整合成同一维度**

**4）把整合后的数据经过网络进行融合**：得出带有3D Box的BV和FV，使用可分离卷积分类？

详解：https://zhuanlan.zhihu.com/p/86312623

### AVOD**

来源：https://zhuanlan.zhihu.com/p/86340957

![](.\image2\AVOD.jpg)

论文题目：Joint 3D Proposal Generation and Object Detection from View Aggregation

开源代码：[https://github.com/kujason/avod](https://link.zhihu.com/?target=https%3A//github.com/kujason/avod)

AVOD和MV3D-Net比较像，前者算是后者的加强版，由于之前我们已经详细介绍了MV3D-Net，所以我们就不对AVOD做完整介绍了，只介绍它做的改进部分，之前MV3D-Net的介绍在下面的连接里。

**改进措施**

**1. 输入做了简化**

**1）去掉了激光点云的前视图输入**

**2）在俯视图中去掉了强度信息**

去掉这两个信息仍然能取得号的效果，就说明俯视图和图像信息已经能够完整诠释三维环境了。

**2. RPN分辨率提高了**

MV3D-Net 是改进的VGG16做特征提取，而AVOD使用的是FPN，它可以在保证特征图相对于输入是全分辨率的，而且还能结合底层细节信息和高层语义信息，因此能显著提高物体特别是小物体的检测效果。

**3. 在数据整合方法上做了改进**

我们先来回忆下MV3D-Net是怎么做的，它先把俯视图中提取的候选区域投影前视图和图像中，然后经过ROI pooling把他们整合成同一个维度，而AVOD使用的是裁剪和调整（crop and resize）。

需要注意的是，由于它使用了全分辨率特征，所以为了在整合时降低维度，先进性了1X1的卷积。

**4. 在3D Bounding Box的编码上添加了几何约束**

MV3D, Axis Aligned, AVOD三种不同的3D Bounding Box编码方式如下图所示，我们可以看到AVOD利用一个底面以及高度约束了3D Bounding Box的几何形状，即要求其为一个长方体，而MV3D只是给出了8个顶点，没有任何的几何约束关系。此外，MV3D中8个顶点需要一个24维（3x8）的向量表示，而AVOD只需要一个10维（2x4+1+1）的向量即可，做到了很好的编码降维工作。

![](.\image2\AVOD-几何约束.jpg)

**5.NMS**

非极大值抑制Non-Maximum Suppression：https://zhuanlan.zhihu.com/p/78504109

### 2019年最新进展

![](.\image2\2019年最新进展.png)

PointRCNN：https://zhuanlan.zhihu.com/p/84335316

Voxel-FPN（上方）：https://blog.csdn.net/wqwqqwqw1231/article/details/103684914

MMF：https://blog.csdn.net/weixin_43278491/article/details/88188783

最新论文：https://zhuanlan.zhihu.com/p/97397273

