# 强化学习



## 强化学习四要素

- 策略
- 收益信号
- 价值函数
- 对环境建立的model（可选）



## 三条主线

- 基于动物心理学的试错法
- 基于最优控制的问题以及使用价值函数和动态规划

- 基于时序差分的方法



# 各种模型

## 多臂赌博机

> 基于第一条主线：试错法
> $$
> Q_t(a)：动作a在时刻t的价值估计(期望)
> $$
>

**1、ε-贪心法**：收敛出所有结果的正态分布

**2、增量式实现**：从
$$
Q_n=\frac{R_1+R_2+...+R_{n-1}}{n-1}
$$
到
$$
Q_{n+1}=Q_n+\frac{1}{n}[R_n-Q_n]
$$


**3、非平稳问题**

> 即收益的概率分布岁时间变化的问题，很多问题都是非平稳问题。
>
> 解决方法：给近期的收益赋予比过去很久的收益更高的权值

$$
Q_{n+1}=Q_n+a[R_n-Q_n]
$$

- 采样平均：a_n(a)=1/n

- 指数近因加权平均：a=常数，a∈(0，1]，指数递减

采样平均满足**随机逼近理论**，可以完全收敛，但是很慢；**第二种方法**，不满足随机逼近理论，永远无法完全收敛，而会随着近期走势变化，但是符合实际应用要求。



**4、乐观初始值**

Q1(a)一般选择0，但是选择一个比较高的初始值，会得到**ε-贪心法**类似的效果。



**5、无偏恒定步长技巧**

> 修正一下Q_n。p35



**6、UCB：置信度上界**

基于UCB，选择非贪婪动作：
$$
A_t=argmax[Q_t(a)+c\sqrt{\frac{lnt}{N_t(a)}}]
$$

> 此方法在现实中，不易应用。



**7、梯度赌博机算法**

> 不估计动作价值，而是利用偏好函数，使用softmax分布来以一种分级的、概率式的方式选择更优的动作。



# 马尔科夫决策链

## 有限MDP

**MDP四元组(S,A,T,R)**

强化学习通常要面临的难题是，对于学习器，MDP四元组并非全部已知，即“无模型” (**model-free**)。最常见的情况是转移函数T未知以及奖赏函数R未知，这时就需要通过在环境中执行动作、观察环境状态的改变和环境给出的奖赏值来学出T和R。



- 分幕式：如：多臂赌博机；适合使用非折扣形式。
- 持续式：上一个状态影响下一个状态；适合使用折扣形式，并且折扣越小，越“短视”。

> 最优性和近似算法：强化学习在线运行的本质使得它在近似最优策略的过程中，对于那些经常出现的状态集合会花更多的努力去学习 好的决策，而代价就是对不经常出现的状态给予的学习力度不够。



MDP就是一种通过**交互式学习**来实现目标的理论框架，有限MDP具有有限状态、动作、收益集合的MDP。

> 重要概念：
>
> 状态、动作、价值
>
> 状态转移收益r(s,a,s')、状态转移概率p(s'|s,a)、
>
> 折扣率（决定未来收益的现值， 并且折扣越小，越“短视”）

在MDP中，我们估计**每个动作a在每个状态s中的价值**q(s,a)，或者估计**给定最优动作下的每个状态的价值**v(s)，

对于估计**单个动作选择的长期结果**来说，这些状态相关的数量是至关重要的。



## POMDP

离散时间POMDP模拟代理与其环境之间的关系。 形式上，POMDP是7元组(S,A,T,R,Ω,O,γ)，其中

- S是一组状态，A是一组动作，
- T(s'|s,a)是状态之间的一组条件转移概率，
- R(s,a,s')是奖励函数。
- Z是一组观察，
- O是一组条件观察概率，
- γ∈[0,1]是折扣因子。

在每个时间段，环境处于某种状态s∈S。The agent在A中采取动作a∈A，这会导致转换到状态s'的环境概率为T(s'|s,a)。同时，代理接收观察o∈Z，它取决于环境的新状态，概率为O(o|s’,a)。最后，代理接收奖励r等于R(s,a,s')。

举例：条件观察概率，就是当你听到一个声音（观察），有概率表示着某个状态，比如汽车从左边或者右边来。

> 熟悉轨迹预测算法：MDP、POMDP、了解MCMC，LDA算法；



## 贝尔曼方程两种形式

> 贝尔曼方程相当于，线性规划的正态方程，由于计算性能不够，不可能求解。
>
> 贝尔曼最优化方程，是最优化价值函数必须满足的一致性条件。

分别记最优策略对应的状态值函数和动作值函数为 ![[公式]](https://www.zhihu.com/equation?tex=V%5E%2A%28s%29+) 和 ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%2A%28s%2C+a%29%E2%80%8B)

最优状态价值函数（当前状态下，采用最优动作的价值）：
$$
V_*^{π}(s) =\max_{a}\sum^{}_{s∈S}{p(s^{'}|s,a)[r(s^{'}|s,a)+γV^π(s^{'})]}
$$
最优动作价值函数：
$$
Q_*^{π}(s,a) =\sum^{}_{s∈S}{p(s^{'}|s,a)[r(s^{'}|s,a)+γ\max_{a'}q_*(s',a')]}
$$

> 贝尔曼方程阐述了一个事实：最优策略下各个状态的价值一定等于这个状态下最优动作的期望回报。



**在** ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%CF%80%28s%2Ca%29%E2%80%8B) **中，不仅策略 π 和初始状态 s 是我们给定的，当前的动作 a 也是我们给定的，这是** ![[公式]](https://www.zhihu.com/equation?tex=Q%5E%CF%80%28s%2Ca%29%E2%80%8B) **和** ![[公式]](https://www.zhihu.com/equation?tex=V%5E%CF%80%28a%29) **的主要区别。**

在得到值函数后，即可以列出MDP的最优策略：
$$
π^* = argmax_πV^{π}(s),(∀s)
$$
即我们的目标是寻找的是在任意初始条件s下，能够最大化值函数的策略π*。



# MDP的三种解法

> MDP基本的解法有三种：
> \- 动态规划法(dynamic programming methods)
> \- 蒙特卡罗方法(Monte Carlo methods)
> \- 时间差分法(temporal difference)

![](.\MDP.jpg)

## 动态规划

**特点**：

> 1、需要有完备的model，如：状态转移概率分布T、回报R
>
> 2、要遍历整个状态集合，计算复杂度高，惧怕维度灾难



一个常用的动态规划模式：

**1、策略评估**：对一个策略的状态价值函数进行迭代计算（最终收敛），以评估这个策略。

**2、策略改进**：给定某个策略的价值函数，计算一个改进的**贪心策略**版本。

> 将这两种方法统一在一起，我们得到了**策略迭代**和**价值迭代**。

**策略迭代**：策略评估->策略改进->...->策略评估->策略不可改进（稳定，最优，终止）。

**价值迭代**：进行一次策略评估，使用公式（4.10）反复迭代，得到最优策略，结合了**策略改进**和**截断策略评估**。

也可以简化为初始值为0，然后反复迭代下面公式。
$$
V_{k+1}(s) =\max_{a}\sum^{}_{s∈S}{p(s^{'}|s,a)[r(s^{'}|s,a)+γV_k(s^{'})]}
$$
价值迭代算法的形式，基于贝尔曼方程。一般多次迭代之后，可以收敛到最优状态值。

了解最佳状态值可能是有用的，特别是评估策略，但它**没有明确地告诉智能体要做什么**。幸运的是，还有Q值迭代版本。

**Q值迭代**：
$$
Q_{k+1}(s,a) =\sum^{}_{s'∈S}{p(s^{'}|s,a)[r(s^{'}|s,a)+γ\max_{a'}Q_k(s^{'},a^{'})]}
$$

> 一旦你有了最佳的 Q 值，定义最优的策略`π*(s)`，就没什么作用了：当智能体处于状态`S`时，它应该选择具有最高 Q 值的动作。



**异步动态规划**

> 传统的DP方法，要遍历整个状态集合S，并对每一个状态进行期望更新操作。每一次操作都基于所有可能的后继状态的价值函数以及它们可能出现的概率。
>
> 异步DP，可以选择重点（减少计算量），**聚焦**于部分常见的状态。



### 广义策略迭代（GPI）

在GPI中，我们同时维护一个近似的策略和近似的价值函数。价值函数会不断迭代使其更加精确地近似对应当前策略的真实价值函数，而当前的策略也会根据当前的价值函数不断调优。这两个过程在一定程度上会互相影响，因为它们互相为对方确定了一个变化的优化目标，但它们整体会使得策略与价值函数趋向最优解。

**策略改进定理可以保证新策略更优，除非已经是最优策略。反过来也保证了策略迭代的流程可以收敛到最优策略和最优价值函数。**



## 蒙特卡洛

**特点**：

> 1、不需要完备的model，只需要足够数量的真实的经验（足以推测出状态转移概率分布的样本）
>
> 2、假设每个状态独立（朴素推断，实际上上下文关联）
>
> 3、价值估计以及策略改进在整个幕结束时才进行。（逐幕改进，而非在线改进）

**优点**：

> 1、不需要完备的model（R和T都不要），可以直接通过与环境的交互来学习最优的决策行为
>
> 2、可以使用数据仿真或采样模型，而且很容易
>
> 3、可以很简单的聚焦于状态的一个小的子集，它可以值评估关注的区域而不评估其余的状态。
>
> 4、在马尔科夫性不成立时，性能损失较小。因为不需要自举。

**蒙特卡洛**：术语，泛指任何包含大量随机成分的估计方法。

**蒙特卡洛方法**：类似之前的赌博机算法（上下文关联的赌博机），并且，从较早状态的视角来看，这个问题是非平稳的。我们已经知道一个状态的价值是从该状态开始的期望回报，即未来的折扣收益累积值的期望。那么一个显而易见的方法是根据经验进行估计，即对所有经过这个状态之后产生的回报进行平均。随着越来越多的回报被观察到，平均值就会收敛于期望值。这一想法是所有蒙特卡洛算法的基础。

蒙特卡洛算法的一个**重要的事实**是：对于每个状态的估计是**独立**的。对于一个状态的估计完全不依赖于其他状态的估计。



**关于是否能够收敛**

> 需要满足两个假设：
>
> 1、试探性出发假设
>
> 2、另一个是在进行策略评估的时候有**无限多**幕的样本序列进行试探



**两种保持试探的方法**：

1、试探性出发（不好）

2、ε-贪心策略



### 同轨策略和离轨策略

同轨：用于**生成采样数据序列的策略**和用于**实际决策的待评估和改进的策略**是相同的。

离轨：用于评估或改进的策略与生成采样数据的策略是不同的。

用来学习的策略被称为**目标策略**，用于生成行动样本的策略被称为**行动策略**。

On-policy的目标策略和行为是同一个策略，其好处就是简单粗暴，直接利用数据就可以优化其策略，但这样的处理会导致策略其实是在学习一个局部最优，因为On-policy的策略没办法很好的同时保持即探索又利用；而Off-policy将目标策略和行为策略分开，可以在保持探索的同时，更能求到全局最优值。但其**难点在于：如何在一个策略下产生的数据来优化另一个策略。**

因此可以利用**重要度采样**。



### 重要度采样

基于一个{S,A}序列对于不同策略发生的相对概率比ρ，即重要度采样比；

这样可以通过执行策略b产生的{S,A}序列的期望回报G，直接来估计策略π产生{S,A}序列的回报。



**5.5之后略**



## 时序差分**

**特点**：

> 1、不需要完备的model，可以直接通过与环境的交互来学习最优的决策行为
>
> 2、每一步采样更新

**优点**：

> 1、TD(0)相比MC，收敛更快
>
> 2、MC在已知数据上表现得更好，但是TD(0)在未来的数据上产生更小的误差
>
> 3、对于状态空间巨大的任务，TD方法可能是唯一可行的逼近确定性等价解的方法

问题研究：P124~126

**参考**

> 增量式更新方法及步长选取，参考：2.4、2.5
>
> G是单次序列回报的实际值，V是用于估计期望的。
>
> 反向传播G和动态规划V
>



**三种方法比较**

一个适用于非平稳环境的简单的每次访问型MC方法（**常量αMC**）可以表示成
$$
V(S_t)=V(S_t)+α[G_t-V(S_t)]
$$
一个最简单的TD方法
$$
V(S_t)=V(S_t)+α[R_{t+1}+γV(S_{t+1})-V(S_t)]
$$
上面的式子中，括号内为需要更新的**误差**。左边为目标，右边为估计值，但并不是完全正确的。

> 见p118
>
> MC的目标之所以是一个“估计值”，是因为公式（6.3）中的期望值是未知的，我们用样本回报来代替实际的期望回报G_t。
>
> DP的目标之所以是一个“估计值”，是因为真实的v_π(S_t+1)是未知，因此要使用当前的估计值V(S_t+1)来代替
>
> TD的目标也是一个“估计值”：1、它**采样**得到对式（6.4）的期望值，2、使用当前的估计值V来代替真实值v_π

**采样更新和期望更新**

TD和MC更新称为**采样更新**，因为它们都会通过采样得到一个后继状态（或SA二元组）

DP方法使用的的**期望更新**，基于所有可能后继节点的完整分布。



### off-policy与on-policy

这种差异有两种解读方式：

1. 策略迭代的策略不是当前交互的策略（Q-learning与Sarsa）
2. 策略迭代时候使用的经验不是以当前策略进行交互的（DQN等具有 experience replay的算法，这个好理解就不解释了）

> sarsa：如果用ε-贪心策略，则计算出的不是最优策略，如果结合离轨策略，需要重要度采样。
>
> 为什么Q学习、DQN不需要重要度采样：因为其更新的就是贪心策略，即目标Q值。



### 双Q学习和最大化偏差



### 近似Q学习和DQN

Q 学习的主要问题是，它**不能很好地扩展到具有许多状态和动作的大（甚至中等）的 MDP**。例如，假如你想用 Q 学习来训练一个智能体去玩 Ms. Pac-Man（图18-1）。Ms. Pac-Man 可以吃超过 150 粒粒子，每一粒都可以存在或不存在（即已经吃过）。因此，可能状态的数目大于 2150 ≈ 1045。空间大小比地球的的总原子数要多得多，所以你绝对无法追踪每一个 Q 值的估计值。

**解决方案是找到一个函数Qθ(s,a)，使用可管理数量的参数（根据矢量θ）来近似 Q 值。**这被称为**近似 Q 学习**。多年来，人们都是手工在状态中提取并线性组合特征（例如，最近的鬼的距离，它们的方向等）来估计 Q 值，但在2013年， DeepMind 表明使用深度神经网络可以工作得更好，特别是对于复杂的问题。它不需要任何特征工程。用于估计 Q 值的 DNN 被称为**深度 Q 网络（DQN）**，并且使用近似 Q 学习的 DQN 被称为深度 Q 学习。

> 很多状态映射对Q(s,a) => 函数Qθ(s,a) => DQN
>
> 只有使用一个参数为θ矢量的函数，足以描述数量庞大的Q(s,a)对



# 函数逼近理论

用一个函数π(s|a, θ)来描述策略，通常来说，权值的数量w远远小于状态的数量，另外改变一个权值将改变许多状态的估计值。因此，当一个状态被更新时，许多其他状态的价值函数也会被更改。这样的泛化可能使学习能力更为强大，但也可能更加难以控制与理解。



**拟合价值函数**

1、回归问题方法

2、DQN



## 策略梯度方法PG

基于参数的π(s|a, θ)的策略函数，可以通过某种性能的度量J(θ)，求得梯度并且不停的优化，满足这些框架的方法，都称为策略梯度方法。

策略梯度定理（分幕式情况）
$$
▽J(θ)∝\sum^{}_{s}μ(s)\sum^{}_{a}q_π(s,a)▽π(a|s,θ)
$$

> 其中：μ是所关注状态的(关于重要性的)概率分布



PG方法输出的是一个动作的概率分布。

**PG核心思想**

> 1、随机一个动作
>
> 2、先计算使用该动作的梯度和衰减后的期望reward
>
> 3、如果奖励为正，应用这些梯度
>
> PS：类比值函数方法的试错法



**PG算法问题**

> 1、如何使用高斯策略，处理连续动作
>
> 2、如何理解MC PG的log求导
>
> 3、AC、A2C、A3C
>
> 4、策略梯度定理证明





# 问题

> 1、评估方式：https://www.jianshu.com/p/734bd7ee0a57
>
> 2、其他：https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-07-PG/



**模仿学习**

Behavior Cloning：数据增强、数据聚合



IRL：

> 1、连续状态空间？
>
> 2、状态转移概率未知？



GAIL：输入state和action分类是否是专家策略

